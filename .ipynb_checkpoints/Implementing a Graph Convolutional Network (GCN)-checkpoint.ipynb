{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9551197290420532\n",
      "Epoch 10, Loss: 0.6276729702949524\n",
      "Epoch 20, Loss: 0.11319617927074432\n",
      "Epoch 30, Loss: 0.02560604177415371\n",
      "Epoch 40, Loss: 0.009924104437232018\n",
      "Epoch 50, Loss: 0.005633097141981125\n",
      "Epoch 60, Loss: 0.003971583675593138\n",
      "Epoch 70, Loss: 0.003149603260681033\n",
      "Epoch 80, Loss: 0.0026519682724028826\n",
      "Epoch 90, Loss: 0.0023040224332362413\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8021208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7760\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1) # get prediction\n",
    "    \n",
    "    # Calculate the accuracy on the test set\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb944082",
   "metadata": {},
   "source": [
    "#### 1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "\n",
    "Over-smoothing: interacting nodes would have quite similar representations; may occur b/c layers aggregate info from neighboring nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b792214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 3-layer GCN\n",
    "class GCN_3_layer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(GCN_3_layer, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec12645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model_3 = GCN_3_layer(input_dim=dataset.num_node_features, hidden_dim1=16, hidden_dim2=16, output_dim=dataset.num_classes)\n",
    "optimizer_3 = optim.Adam(model_3.parameters(), lr=0.01)\n",
    "criterion_3 = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fbbb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9486275911331177\n",
      "Epoch 10, Loss: 0.9624478816986084\n",
      "Epoch 20, Loss: 0.24392223358154297\n",
      "Epoch 30, Loss: 0.020068548619747162\n",
      "Epoch 40, Loss: 0.003072653664276004\n",
      "Epoch 50, Loss: 0.0010185071732848883\n",
      "Epoch 60, Loss: 0.0006181305507197976\n",
      "Epoch 70, Loss: 0.00047641972196288407\n",
      "Epoch 80, Loss: 0.0004055898170918226\n",
      "Epoch 90, Loss: 0.00036247126990929246\n",
      "Training complete for the 3-layer GCN!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_3.train()\n",
    "    optimizer_3.zero_grad()\n",
    "\n",
    "    out = model_3(data)\n",
    "    loss = criterion_3(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_3.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete for the 3-layer GCN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d67dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7630\n"
     ]
    }
   ],
   "source": [
    "model_3.eval()\n",
    "\n",
    "with torch.no_grad(): \n",
    "    out = model_3(data)\n",
    "    pred = out.argmax(dim=1) # get prediction\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68184f02",
   "metadata": {},
   "source": [
    "As we add another layer to the GCN, the test accuracy drops slightly from 0.7760 to 0.7630. This suggest that adding more layers to the GCN did not improve the model's performance on the test set. \n",
    "\n",
    "In my opinion, this drop may be due to the over-smoothing problem, as increasing the number of the GCN layers without regularization may cause the node embeddings to become too similar to each other. This problem may lead to the model being less capable of distinguishing between nodes in different classes. This may be improved by adding regularizations like dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b603ac",
   "metadata": {},
   "source": [
    "#### 2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b3f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large_hidden = GCN(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer_large_hidden = optim.Adam(model_large_hidden.parameters(), lr=0.01)\n",
    "criterion_large_hidden = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a099583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9551746845245361\n",
      "Epoch 10, Loss: 0.07922256737947464\n",
      "Epoch 20, Loss: 0.00296090729534626\n",
      "Epoch 30, Loss: 0.0005492267664521933\n",
      "Epoch 40, Loss: 0.0002485028235241771\n",
      "Epoch 50, Loss: 0.00017058693629223853\n",
      "Epoch 80, Loss: 0.00012135658471379429\n",
      "Epoch 90, Loss: 0.00011492427438497543\n",
      "Training complete for GCN w/ larger hidden layer!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_large_hidden.train()\n",
    "    optimizer_large_hidden.zero_grad()\n",
    "    \n",
    "    out = model_large_hidden(data)\n",
    "    loss = criterion_large_hidden(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_large_hidden.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete for GCN w/ larger hidden layer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e7c65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with larger hidden dimension: 0.7840\n"
     ]
    }
   ],
   "source": [
    "model_large_hidden.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_large_hidden(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "    \n",
    "print(f'Test Accuracy with larger hidden dimension: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117c187",
   "metadata": {},
   "source": [
    "The test accuracy of model with larger hidden dimension  0.7840 shows a slight improvement over both the 2-layer GCN with 16 hidden units (Test Accuracy: 0.7760) and the 3-layer GCN with 16 hidden units (Test Accuracy: 0.7630). This indicates that by increasing the number of hidden dimensions, the model gained **more capacity** to learn more complexed representations of the features, which allows the model to capture more details in the data. \n",
    "\n",
    "In addition, increasing the hidden dimension in this particular case does not worsen the model performance by over-smoothing or overfitting. However, there is an associated increase in computational cost as there are increased number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd848b",
   "metadata": {},
   "source": [
    "#### 3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a324508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_sigmoid(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN_sigmoid, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb9eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sigmoid = GCN_sigmoid(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer_sigmoid = optim.Adam(model_sigmoid.parameters(), lr=0.01)\n",
    "criterion_sigmoid = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5fa0d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.265245199203491\n",
      "Epoch 10, Loss: 1.525286316871643\n",
      "Epoch 20, Loss: 1.0788142681121826\n",
      "Epoch 30, Loss: 0.7327616810798645\n",
      "Epoch 40, Loss: 0.4851303696632385\n",
      "Epoch 50, Loss: 0.3223065435886383\n",
      "Epoch 60, Loss: 0.22101959586143494\n",
      "Epoch 70, Loss: 0.15894536674022675\n",
      "Epoch 80, Loss: 0.12027529627084732\n",
      "Epoch 90, Loss: 0.09509587287902832\n",
      "Training complete for GCN w/ sigmoid activation!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_sigmoid.train()\n",
    "    optimizer_sigmoid.zero_grad()\n",
    "    \n",
    "    out = model_sigmoid(data)\n",
    "    loss = criterion_sigmoid(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_sigmoid.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "print(\"Training complete for GCN w/ sigmoid activation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da398167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with sigmoid activation: 0.7690\n"
     ]
    }
   ],
   "source": [
    "model_sigmoid.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_sigmoid(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "    \n",
    "print(f'Test Accuracy with sigmoid activation: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92542e90",
   "metadata": {},
   "source": [
    "The test accuracy with sigmoid activation function is slightly lower than with ReLU. This may be attributed to the vanishing gradient problem, which slows down the learning process and lead to slight decrease in model accuracy. On the other hand, ReLu allows for more sparsity and sets some neurons to output zero, which helps prevent the vanishing gradient problem. \n",
    "\n",
    "Moreover, I noticed that the training time of the model with sigmoid activation is longer than the one with ReLU activation. This could be attributed to the lower computational complexity of ReLU than sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e64333",
   "metadata": {},
   "source": [
    "#### 4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1322f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.1\n",
    "num_train_nodes = int(train_ratio * data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6526e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the node indices (this was suggested by ChatGPT)\n",
    "perm = torch.randperm(data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c79184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[perm[:num_train_nodes]] = True\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask[perm[num_train_nodes:]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a2036b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e35507e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10_pct = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer_10_pct = optim.Adam(model_10_pct.parameters(), lr=0.01)\n",
    "criterion_10_pct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dde74a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9473978281021118\n",
      "Epoch 10, Loss: 0.7699562907218933\n",
      "Epoch 20, Loss: 0.20229144394397736\n",
      "Epoch 30, Loss: 0.05703773722052574\n",
      "Epoch 40, Loss: 0.021402880549430847\n",
      "Epoch 50, Loss: 0.0108838165178895\n",
      "Epoch 60, Loss: 0.007085033226758242\n",
      "Epoch 70, Loss: 0.0052841054275631905\n",
      "Epoch 80, Loss: 0.004255638923496008\n",
      "Epoch 90, Loss: 0.003575586946681142\n",
      "Training complete for GCN w/ 10% training nodes!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_10_pct.train()\n",
    "    optimizer_10_pct.zero_grad()\n",
    "    \n",
    "    out = model_10_pct(data)\n",
    "    loss = criterion_10_pct(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_10_pct.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "print(\"Training complete for GCN w/ 10% training nodes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44360394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with 10% training nodes: 0.8084\n"
     ]
    }
   ],
   "source": [
    "model_10_pct.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_10_pct(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "    \n",
    "print(f'Test Accuracy with 10% training nodes: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93c7b7",
   "metadata": {},
   "source": [
    "The test accuracy of the model trained on 10% of the nodes and test on the remaining 90% is higher than any other models constructed before. In my opinion, this could be due to several factors:\n",
    "\n",
    "- The model may generalize better when trained on fewer nodes, because it is less likely to overfit on the training set.\n",
    "- The graph structure of the dataset may be well-connected and informative enough that even a small number of training nodes is enough for the model to generalize well for the rest of the nodes.  \n",
    "\n",
    "These hypotheses can be further validated through permutation testing, where he nodes are shuffled multiple times and different 10% nodes are used to train the model every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612b47e",
   "metadata": {},
   "source": [
    "#### 5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "271ac334",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask = dataset[0].train_mask\n",
    "data.test_mask = dataset[0].test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0b7d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rmsprop = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer_rmsprop = optim.RMSprop(model_rmsprop.parameters(), lr=0.01)\n",
    "criterion_rmsprop = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9e1c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9377018213272095\n",
      "Epoch 10, Loss: 0.035184767097234726\n",
      "Epoch 20, Loss: 0.015747353434562683\n",
      "Epoch 30, Loss: 0.009820069186389446\n",
      "Epoch 40, Loss: 0.006943057291209698\n",
      "Epoch 50, Loss: 0.00526403496041894\n",
      "Epoch 60, Loss: 0.004169971216470003\n",
      "Epoch 70, Loss: 0.003405557246878743\n",
      "Epoch 80, Loss: 0.0028436449356377125\n",
      "Epoch 90, Loss: 0.0024153452832251787\n",
      "Training complete for GCN w/ RMSprop optimizer!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_rmsprop.train()\n",
    "    optimizer_rmsprop.zero_grad()\n",
    "    \n",
    "    out = model_rmsprop(data)\n",
    "    loss = criterion_rmsprop(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_rmsprop.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "print(\"Training complete for GCN w/ RMSprop optimizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2d98e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with RMSprop optimizer: 0.7820\n"
     ]
    }
   ],
   "source": [
    "model_rmsprop.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_rmsprop(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Test Accuracy with RMSprop optimizer: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2dae8",
   "metadata": {},
   "source": [
    "The difference between the test accuracy of the models with Adam and RMSprop is small, meaning that both optimizers perform fairly well for this task. \n",
    "\n",
    "In general, RMSprop tends to converge more slowly compared to Adam. This may lead to slightly less efficiency in updating weights and finding the optimal solution. (This paragraph is written with the help of ChatGPT.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a870e",
   "metadata": {},
   "source": [
    "### Extra credit: \n",
    "#### 1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bce6d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ba78230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_with_weights(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN_with_weights, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        return torch.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "381c614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weighted = GCN_with_weights(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer_weighted = optim.Adam(model_weighted.parameters(), lr=0.01)\n",
    "criterion_weighted = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a20b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9426648616790771\n",
      "Epoch 10, Loss: 0.5306446552276611\n",
      "Epoch 20, Loss: 0.08349411189556122\n",
      "Epoch 30, Loss: 0.017982272431254387\n",
      "Epoch 40, Loss: 0.006782583426684141\n",
      "Epoch 50, Loss: 0.003893067594617605\n",
      "Epoch 60, Loss: 0.0028360977303236723\n",
      "Epoch 70, Loss: 0.0023138488177210093\n",
      "Epoch 80, Loss: 0.0019932996947318316\n",
      "Epoch 90, Loss: 0.0017628786154091358\n",
      "Training complete for GCN w/ edge weights!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_weighted.train()\n",
    "    optimizer_weighted.zero_grad()\n",
    "    \n",
    "    out = model_weighted(data)\n",
    "    loss = criterion_weighted(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_weighted.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "print(\"Training complete for GCN w/ edge weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48ba5b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with edge weights: 0.7880\n"
     ]
    }
   ],
   "source": [
    "model_weighted.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_weighted(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    accuracy = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Test Accuracy with edge weights: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5801e",
   "metadata": {},
   "source": [
    "When we use binary adjacency matrices in GCNs, the information passed between nodes is binary, meaning there is either a connection (1) or no connection (0). On the other hand, if we use edge weights (non-binary) in the adjacency matrix, the information aggregated by each node from its neighbors will be weighted by the strength of the connection between nodes. This means that during the message passing process, nodes connected by stronger edges would contribute more to the final representation of a node. \n",
    "\n",
    "By using the edge weight in adjacency matrix, the model may capture more details about the relationships between nodes and better learn the features, leading to richer node embeddings and potentially improving the model performance. This may also introduce more complexity in the model learning process, which can be suitable for complex network or graph structures but also lead to higher computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ba876",
   "metadata": {},
   "source": [
    "#### 2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a604dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_no_logsoftmax(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN_no_logsoftmax, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385562cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_logsoftmax = GCN_no_logsoftmax(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer_no_logsoftmax = optim.Adam(model_no_logsoftmax.parameters(), lr=0.01)\n",
    "criterion_no_logsoftmax = nn.CrossEntropyLoss()  # This already applies softmax internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef93321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.953733205795288\n",
      "Epoch 10, Loss: 0.5637298226356506\n",
      "Epoch 20, Loss: 0.08987681567668915\n",
      "Epoch 30, Loss: 0.01918867975473404\n",
      "Epoch 40, Loss: 0.007057531271129847\n",
      "Epoch 50, Loss: 0.0039271279238164425\n",
      "Epoch 60, Loss: 0.0028138982597738504\n",
      "Epoch 70, Loss: 0.002280070213600993\n",
      "Epoch 80, Loss: 0.0019572952296584845\n",
      "Epoch 90, Loss: 0.0017287489026784897\n",
      "Training complete for GCN w/o log softmax!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model_no_logsoftmax.train()\n",
    "    optimizer_no_logsoftmax.zero_grad()\n",
    "    \n",
    "    out = model_no_logsoftmax(data)\n",
    "    \n",
    "    loss = criterion_no_logsoftmax(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_no_logsoftmax.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete for GCN w/o log softmax!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce118451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fecbada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (first 5 training nodes):\n",
      "tensor([[-3.2499, -1.0346, -4.3134,  6.3326, -2.1912, -1.8376, -5.6061],\n",
      "        [-4.8734, -3.7940,  0.3320, -3.4253, 11.6896, -4.2241, -4.5810],\n",
      "        [-6.3840, -3.6482, -0.7484,  0.0278,  9.3596, -3.5577, -5.5921],\n",
      "        [ 7.7711, -3.0735, -2.5449, -4.2608, -3.7189, -3.7195, -0.4565],\n",
      "        [-3.7300, -1.3807, -2.6438,  5.7616, -2.0244, -1.7527, -6.1350]])\n",
      "\n",
      "Probabilities (after applying softmax, first 5 training nodes):\n",
      "tensor([[6.8841e-05, 6.3083e-04, 2.3765e-05, 9.9879e-01, 1.9843e-04, 2.8262e-04,\n",
      "         6.5244e-06],\n",
      "        [6.4088e-08, 1.8861e-07, 1.1680e-05, 2.7269e-07, 9.9999e-01, 1.2268e-07,\n",
      "         8.5853e-08],\n",
      "        [1.4540e-07, 2.2425e-06, 4.0750e-05, 8.8557e-05, 9.9987e-01, 2.4549e-06,\n",
      "         3.2101e-07],\n",
      "        [9.9965e-01, 1.9503e-05, 3.3089e-05, 5.9493e-06, 1.0228e-05, 1.0223e-05,\n",
      "         2.6709e-04],\n",
      "        [7.5328e-05, 7.8930e-04, 2.2320e-04, 9.9795e-01, 4.1466e-04, 5.4411e-04,\n",
      "         6.7994e-06]])\n",
      "\n",
      "Sum of probabilities:\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "model_no_logsoftmax.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_no_logsoftmax(data)\n",
    "\n",
    "# Print the output for a few nodes\n",
    "print(\"Output (first 5 training nodes):\")\n",
    "print(out[data.train_mask][:5])\n",
    "\n",
    "# Apply softmax to the output to convert them into probabilities\n",
    "probabilities = F.softmax(out, dim=1)\n",
    "\n",
    "# Print the probabilities for the same nodes\n",
    "print(\"\\nProbabilities (after applying softmax, first 5 training nodes):\")\n",
    "print(probabilities[data.train_mask][:5])\n",
    "\n",
    "# Check the sum of probabilities\n",
    "print(\"\\nSum of probabilities:\")\n",
    "print(probabilities[data.train_mask][:5].sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ede6a",
   "metadata": {},
   "source": [
    "If the log-softmax function is removed from the output layer of the model, the model will output logits, which is the raw scores each class. When we apply softmax manually on these numbers and convert them into probabilities, they sum up to one for each node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00108f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually calculated NLL loss: 0.0007521277293562889\n",
      "Loss from nn.CrossEntropyLoss: 0.0007521277293562889\n"
     ]
    }
   ],
   "source": [
    "train_nodes = data.train_mask.nonzero(as_tuple=True)[0][:5]\n",
    "\n",
    "# Manually calculate the loss w/ softmax and NLL\n",
    "log_probs = F.log_softmax(out[train_nodes], dim=1)  # Log-softmax\n",
    "nll_loss_manual = -log_probs[range(5), data.y[train_nodes]].mean()  # NLL\n",
    "\n",
    "print(f\"Manually calculated NLL loss: {nll_loss_manual.item()}\")\n",
    "\n",
    "# Compare with nn.CrossEntropyLoss\n",
    "nll_loss_crossentropy = criterion_no_logsoftmax(out[train_nodes], data.y[train_nodes])\n",
    "\n",
    "print(f\"Loss from nn.CrossEntropyLoss: {nll_loss_crossentropy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63894d",
   "metadata": {},
   "source": [
    "After manually computing the negative log-likelihood loss from the output logits and comparing them with the output of CrossEntropyLoss, they are the same. This indicates that `nn.CrossEntropyLoss` will internally apply the softmax function to convert the logits into probabilities before computing the loss. Therefore, the loss function will still run correctly even if we remove softmax from the model output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9109d2",
   "metadata": {},
   "source": [
    "## No points, just for you to think about:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6d3ca",
   "metadata": {},
   "source": [
    "#### 1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35bda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b834a86",
   "metadata": {},
   "source": [
    "#### 2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667c5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b72f5e1d",
   "metadata": {},
   "source": [
    "#### 3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3430dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711968c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
